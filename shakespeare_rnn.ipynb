{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Resurrecting Shakespeare with LTSM recurrent neural networks\n",
    " Graeme West - 2019-04-28\n",
    " \n",
    " ![Shakespeare Meme](https://i.pinimg.com/236x/f6/11/27/f611272ef416195a7726a037d4394de4.jpg \"Shakespere Meme\")\n",
    " \n",
    "This notebook contains an version of the 'Generating Text in the Style of an Example' chapter from Douwe Osinga's [Deep Learning Cookbook](http://shop.oreilly.com/product/0636920097471.do) (O'Reilly, 2018).\n",
    " \n",
    "The notebook imports the Complete Works of Shakespeare from Project Gutenberg, builds a recurrent neural network model with LTSM layers. The resulting text is somewhat convincing to a casual observer, as long as they aren't a paying too much attention:\n",
    "\n",
    "\n",
    "> Thou smother'st honesty, thou murther'st troth;\n",
    ">\n",
    "> Thou foul abettor! thou notorious bawd!\n",
    ">\n",
    "> Thou plantest scandal and displacest laud:\n",
    ">\n",
    "> Thou ravisher, thou t#he more subject of thy love,\n",
    ">\n",
    "> And therefore thou art a soldier to the state,\n",
    ">\n",
    "> And therefore we have seen the strange seasons,\n",
    ">\n",
    "> And therefore we have seen the streets of the world,\n",
    ">\n",
    "> And therefore we have seen the state of men,\n",
    ">\n",
    "> And therefore we have seen the state of men,\n",
    ">\n",
    "> And therefore we have seen the state of men,\n",
    ">\n",
    "> And therefore we have seen the state of men,\n",
    ">\n",
    "> And therefore we have seen \n",
    "\n",
    "As you can see, the results can sometimes contain amusingly Elizabethan profanities! Because it's a character-level network, and it's trained on Shakespeare's plays, there are definitely oddities creeping in. Notably, characters used for dramatic prompts (such as the hashmark, used to precede the names of characters in the play, and stage directions), are replicated in the middle of prose.\n",
    "\n",
    "Also, the output gets more repetitive the longer it freestyles from the 'seed' text (the starting point in the real corpus).\n",
    "\n",
    "The training process took around 20 hours to run 12 epochs through the training data on my MacBook Pro 2016 Core i7. Just for fun, I also tried out the [TensorFlow Cloud TPU demo for a very similar Shakespeare-generating RNN](https://colab.research.google.com/drive/1DWdpYrgDB9cAMj4o2lTjROGLiXxzUFFf). Training on the TPUs took something like four minutes! I guess this shows the power of massively-parallel vector processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    GUTENBERG = True\n",
    "    from gutenberg.acquire import load_etext\n",
    "    from gutenberg.query import get_etexts, get_metadata\n",
    "    from gutenberg.acquire import get_metadata_cache\n",
    "    from gutenberg.acquire.text import UnknownDownloadUriException\n",
    "    from gutenberg.cleanup import strip_headers\n",
    "    from gutenberg._domain_model.exceptions import CacheAlreadyExistsException\n",
    "except ImportError:\n",
    "    GUTENBERG = False\n",
    "    print('Gutenberg is not installed. See instructions at https://pypi.python.org/pypi/Gutenberg')\n",
    "from keras.models import Input, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "import keras.callbacks\n",
    "import keras.backend as K\n",
    "import scipy.misc\n",
    "import json\n",
    "\n",
    "import os, sys\n",
    "import re\n",
    "import PIL\n",
    "from PIL import ImageDraw\n",
    "\n",
    "from keras.optimizers import RMSprop\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.utils import get_file\n",
    "\n",
    "from IPython.display import clear_output, Image, display, HTML\n",
    "try:\n",
    "    from io import BytesIO\n",
    "except ImportError:\n",
    "    from StringIO import StringIO as BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "cache = get_metadata_cache()\n",
    "try:\n",
    "    cache.populate()\n",
    "except CacheAlreadyExistsException:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 The Complete Works of William Shakespeare\n",
      "10281 Antony's Address over the Body of Caesar\r\n",
      "From Julius Caesar\n",
      "10606 The Tragedie of Hamlet, Prince of Denmark\n",
      "A Study with the Text of the Folio of 1623\n",
      "1041 Shakespeare's Sonnets\n",
      "1045 Venus and Adonis\n"
     ]
    }
   ],
   "source": [
    "if GUTENBERG:\n",
    "    for text_id in get_etexts('author', 'Shakespeare, William'):\n",
    "        print(text_id, list(get_metadata('title', text_id))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5518999"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if GUTENBERG:\n",
    "    shakespeare = strip_headers(load_etext(100))\n",
    "else:\n",
    "    path = get_file('shakespeare', 'https://storage.googleapis.com/deep-learning-cookbook/100-0.txt')\n",
    "    shakespeare = open(path).read()\n",
    "training_text = shakespeare.split('\\nTHE END', 1)[-1]\n",
    "len(training_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = list(sorted(set(training_text)))\n",
    "char_to_idx = {ch: idx for idx, ch in enumerate(chars)}\n",
    "len(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def char_rnn_model(num_chars, num_layers, num_nodes=512, dropout=0.1):\n",
    "    input = Input(shape=(None, num_chars), name='input')\n",
    "    prev = input\n",
    "    for i in range(num_layers):\n",
    "        lstm = LSTM(num_nodes, return_sequences=True, name='lstm_layer_%d' % (i + 1))(prev)\n",
    "        if dropout:\n",
    "            prev = Dropout(dropout)(lstm)\n",
    "        else:\n",
    "            prev = lstm\n",
    "    dense = TimeDistributed(Dense(num_chars, name='dense', activation='softmax'))(prev)\n",
    "    model = Model(inputs=[input], outputs=[dense])\n",
    "    optimizer = RMSprop(lr=0.01)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, None, 95)          0         \n",
      "_________________________________________________________________\n",
      "lstm_layer_1 (LSTM)          (None, None, 640)         1884160   \n",
      "_________________________________________________________________\n",
      "lstm_layer_2 (LSTM)          (None, None, 640)         3279360   \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, None, 95)          60895     \n",
      "=================================================================\n",
      "Total params: 5,224,415\n",
      "Trainable params: 5,224,415\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = char_rnn_model(len(chars), num_layers=2, num_nodes=640, dropout=0)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.]]]),\n",
       " array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 1., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]],\n",
       " \n",
       "        [[0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0., ..., 0., 0., 0.],\n",
       "         [0., 0., 1., ..., 0., 0., 0.],\n",
       "         [0., 0., 0., ..., 0., 0., 0.]]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHUNK_SIZE = 160\n",
    "\n",
    "def data_generator(all_text, char_to_idx, batch_size, chunk_size):\n",
    "    X = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    y = np.zeros((batch_size, chunk_size, len(char_to_idx)))\n",
    "    while True:\n",
    "        for row in range(batch_size):\n",
    "            idx = random.randrange(len(all_text) - chunk_size - 1)\n",
    "            chunk = np.zeros((chunk_size + 1, len(char_to_idx)))\n",
    "            for i in range(chunk_size + 1):\n",
    "                chunk[i, char_to_idx[all_text[idx + i]]] = 1\n",
    "            X[row, :, :] = chunk[:chunk_size]\n",
    "            y[row, :, :] = chunk[1:]\n",
    "        yield X, y\n",
    "\n",
    "next(data_generator(training_text, char_to_idx, 4, chunk_size=CHUNK_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Epoch 1/40\n",
      " - 6085s - loss: 3.3449 - acc: 0.1996\n",
      "Epoch 2/40\n",
      " - 5532s - loss: 3.0624 - acc: 0.2363\n",
      "Epoch 3/40\n",
      " - 5923s - loss: 2.1259 - acc: 0.4595\n",
      "Epoch 4/40\n",
      " - 5934s - loss: 1.9516 - acc: 0.5120\n",
      "Epoch 5/40\n",
      " - 5923s - loss: 1.9098 - acc: 0.5252\n",
      "Epoch 6/40\n",
      " - 5945s - loss: 1.8549 - acc: 0.5388\n",
      "Epoch 7/40\n",
      " - 5955s - loss: 1.8840 - acc: 0.5361\n",
      "Epoch 8/40\n",
      " - 5936s - loss: 1.8322 - acc: 0.5476\n",
      "Epoch 9/40\n",
      " - 5967s - loss: 1.7672 - acc: 0.5613\n",
      "Epoch 10/40\n",
      " - 6077s - loss: 1.8327 - acc: 0.5513\n",
      "Epoch 11/40\n",
      " - 5983s - loss: 1.7622 - acc: 0.5653\n",
      "Epoch 12/40\n",
      " - 5943s - loss: 1.8123 - acc: 0.5581\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb2eefd4a8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early = keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.03,\n",
    "                              patience=3,\n",
    "                              verbose=0, mode='auto')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "model.fit_generator(\n",
    "    data_generator(training_text, char_to_idx, batch_size=BATCH_SIZE, chunk_size=CHUNK_SIZE),\n",
    "    epochs=40,\n",
    "    callbacks=[early,],\n",
    "    steps_per_epoch=2 * len(training_text) / (BATCH_SIZE * CHUNK_SIZE),\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('shakespeare.json', 'w') as fout:\n",
    "    json.dump({\n",
    "        'chars': ''.join(chars),\n",
    "        'char_to_idx': char_to_idx,\n",
    "        'chunk_size': CHUNK_SIZE,\n",
    "    }, fout)\n",
    "model.save('shakespeare.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aw'd;\n",
      "Thou smother'st honesty, thou murther'st troth;\n",
      "Thou foul abettor! thou notorious bawd!\n",
      "Thou plantest scandal and displacest laud:\n",
      "  Thou ravisher, thou t#he more subject of thy love,\n",
      "  And therefore thou art a soldier to the state,\n",
      "  And therefore we have seen the strange seasons,\n",
      "  And therefore we have seen the streets of the world,\n",
      "  And therefore we have seen the state of men,\n",
      "  And therefore we have seen the state of men,\n",
      "  And therefore we have seen the state of men,\n",
      "  And therefore we have seen the state of men,\n",
      "  And therefore we have seen \n"
     ]
    }
   ],
   "source": [
    "def generate_output(model, training_text, start_index=None, diversity=None, amount=400):\n",
    "    if start_index is None:\n",
    "        start_index = random.randint(0, len(training_text) - CHUNK_SIZE - 1)\n",
    "    generated = training_text[start_index: start_index + CHUNK_SIZE]\n",
    "    yield generated + '#'\n",
    "    for i in range(amount):\n",
    "        x = np.zeros((1, len(generated), len(chars)))\n",
    "        for t, char in enumerate(generated):\n",
    "            x[0, t, char_to_idx[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        if diversity is None:\n",
    "            next_index = np.argmax(preds[len(generated) - 1])\n",
    "        else:\n",
    "            preds = np.asarray(preds[len(generated) - 1]).astype('float64')\n",
    "            preds = np.log(preds) / diversity\n",
    "            exp_preds = np.exp(preds)\n",
    "            preds = exp_preds / np.sum(exp_preds)\n",
    "            probas = np.random.multinomial(1, preds, 1)\n",
    "            next_index = np.argmax(probas)     \n",
    "        next_char = chars[next_index]\n",
    "        yield next_char\n",
    "\n",
    "        generated += next_char\n",
    "    return generated\n",
    "\n",
    "for ch in generate_output(model, training_text):\n",
    "    sys.stdout.write(ch)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scene V. Another room in the same.\n",
      "\n",
      "\n",
      "ACT III\n",
      "Scene I. Florence. A room in the Duke’s palace.\n",
      "Scene II. Rossillon. A room in the Countess’s palace.\n",
      "Scene III. Fl#orence. A room in the Castle.\n",
      "\n",
      "\n",
      "ACT I\n",
      "\n",
      "SCENE I. A room in the Castle.\n",
      "\n",
      " Enter the King and Parolles and Attendants.\n",
      "\n",
      "LUCENTIO.\n",
      "I will not stay the letter to your lordship.\n",
      "\n",
      "POLONIUS.\n",
      "I am a gentleman of the strange song that I have seen the strange service\n",
      "Of the which I have seen the state of men.\n",
      "\n",
      " [_Exeunt all but Cassius._]\n",
      "\n",
      "CASSIUS.\n",
      "I will not stay the word of me.\n",
      "\n",
      "BRUTUS.\n",
      "What is the matter?\n"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, training_text, start_index=398, diversity=None, amount=400):\n",
    "    sys.stdout.write(ch)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gave to Alexander; to Ptolemy he assign'd\n",
      "    Syria, Cilicia, and Phoenicia. She\n",
      "    In th' habiliments of the goddess Isis\n",
      "    That day appear'd; and oft befor#e the King\n",
      "    The more the which they shall be so bold to see them\n",
      "    That they are so far of state as they were all the world.\n",
      "    The strange season of the world is strong,\n",
      "    And therefore will I be so far to see the streets\n",
      "    That they are so far of strange and so far\n",
      "    As the season where they are seen to see them.\n",
      "    The strange suit of the world is so far the strangers\n",
      "    That they\n"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, training_text):\n",
    "    sys.stdout.write(ch)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d midnight still,\n",
      "    Guarded with grandsires, babies, and old women,\n",
      "    Either past or not arriv'd to pith and puissance;\n",
      "    For who is he whose chin is but #a shame?\n",
      "    The strange season of the world is strong,\n",
      "    And therefore will I be so far to see the streets\n",
      "    That they are so far of strange and so far\n",
      "    As the season where they are seen to see them.\n",
      "    The strange season of the world is strong,\n",
      "    And therefore will I be so far to see the streets\n",
      "    That they are so far of strange and so far\n",
      "    As the season where they are seen to see\n"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, training_text):\n",
    "    sys.stdout.write(ch)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [A trumpet within.] What trumpet is that\n",
      "    same?\n",
      "  IAGO. Something from Venice, sure. 'Tis Lodovico\n",
      "    Come from the Duke. And, see your wife is with him.\n",
      "\n",
      "#                                                                                                                                                                                                                                                                                                                                                                                                                \n"
     ]
    }
   ],
   "source": [
    "for ch in generate_output(model, training_text):\n",
    "    sys.stdout.write(ch)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
